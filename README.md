# Awesome Quantization Side Effects [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

Quantization improves efficiency but may compromise trustworthiness and model fairness. 
This repo contains a curated list of papers and resources focused on the undesired effects of model quantization, including its impact on:

- Fairness  
- Robustness  
- Calibration  
- Toxicity and safety.

Contributions are welcome — feel free to open a PR with missing papers.

## Paper list

| Title | Link |
|-------|------|
| **Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression** (ICML 2024) | [arXiv](https://arxiv.org/abs/2403.15447) |
| **Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression** (EMNLP Findings 2024) | [ACL Anthology](https://aclanthology.org/2024.findings-emnlp.901/) |
| **Benchmarking Post-Training Quantization in LLMs: A Comprehensive Taxonomy** (Feb 2025) | [arXiv](https://arxiv.org/abs/2502.13178) |
| **A Comprehensive Evaluation of Quantization Strategies for Large Language Models** (ACL Findings 2024) | [ACL Anthology](https://aclanthology.org/2024.findings-acl.726/) |
| **BiLLM: Pushing the Limit of Post-Training Quantization for LLMs** (Feb 2024) | [arXiv](https://arxiv.org/abs/2402.04291) |
| **When Quantization Affects Confidence of Large Language Models?** (NAACL Findings 2024) | [ACL Anthology](https://aclanthology.org/2024.findings-naacl.124/) |
| **On the Impact of Calibration Data in Post‑Training Quantization and Pruning** (ACL 2023) | [arXiv](https://arxiv.org/abs/2311.09755) |
| **Self‑calibration for Language Model Quantization and Pruning** (NAACL 2025) | [arXiv](https://arxiv.org/abs/2410.17170) |
| **How Does Quantization Affect Multilingual LLMs?** (EMNLP Findings 2024)| [ACL Anthology](https://aclanthology.org/2024.findings-emnlp.935/) |

